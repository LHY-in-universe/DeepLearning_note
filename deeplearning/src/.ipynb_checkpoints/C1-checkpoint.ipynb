{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd37e7a-1bb4-489b-b643-a0ddd06d8b7f",
   "metadata": {},
   "source": [
    "# 1.2 Exercise \n",
    "1. For a perceptron, the positive part is always 1 and negetive part is always 0, times a positive number could nor change their sign.\n",
    "2. $w \\times x+b \\longrightarrow c (x \\times x+b)\\longrightarrow\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c4933-182b-4209-a933-02076fb88b69",
   "metadata": {},
   "source": [
    "# 1.4 Execcise\n",
    "1. I think its same with 10digits to 2digits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e0e68c-d253-499b-950c-b7390352fe93",
   "metadata": {},
   "source": [
    "# 1.5\n",
    "The gradient descent algorithm\n",
    "\n",
    "\n",
    " $C(w,b)\\equiv\\frac{1}{2n}\\sum_x\\|y(x)-a\\|^2$\n",
    "\n",
    "\n",
    "$ \\Delta C\\approx\\frac{\\partial C}{\\partial\\nu_1}\\Delta\\nu_1+\\frac{\\partial C}{\\partial\\nu_2}\\Delta\\nu_2$  \n",
    "\n",
    "$\\Delta C\\approx\\nabla C\\cdot\\Delta\\nu$\n",
    "\n",
    "$\\Delta\\nu=-\\eta\\nabla C$\n",
    "\n",
    "\n",
    "$\\nu\\to\\nu^{\\prime}=\\nu-\\eta\\nabla C$\n",
    "# Exercise\n",
    "1. Cauchy-Schwarz Inequality $|\\langle\\mathbf{u},\\mathbf{v}\\rangle|\\leq\\|\\mathbf{u}\\|\\cdot\\|\\mathbf{v}\\|$\n",
    "\n",
    "I think it like a Inverse function.\n",
    "\n",
    "2. Quadratic Function\n",
    "\n",
    "\n",
    "# stochastic gradient descent\n",
    "Chose a mini-batch to calculate $C $\n",
    "\n",
    "\n",
    "$\\frac{\\sum_{j=1}^m\\nabla C_{X_j}}{m}\\approx\\frac{\\sum_x\\nabla C_x}{n}=\\nabla C$\n",
    "\n",
    "\n",
    " $w_k\\quad\\to\\quad w_k^{\\prime}=w_k-\\frac{\\eta}{m}\\sum_j\\frac{\\partial C_{X_j}}{\\partial w_k}$\n",
    " \n",
    " \n",
    " $b_l\\quad\\to\\quad b_l^{\\prime}=b_l-\\frac{\\eta}{m}\\sum_j\\frac{\\partial C_{X_j}}{\\partial b_l},$\n",
    "\n",
    "\n",
    "  # Exercise \n",
    "   Fast but inaccurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26c8229a-924f-49f4-8a86-a59161794168",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T05:18:41.123023Z",
     "start_time": "2025-04-27T05:18:41.099312Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "    \"\"\"\n",
    "    全连接前馈神经网络实现\n",
    "    支持反向传播和随机梯度下降训练\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes):\n",
    "        \"\"\"\n",
    "        初始化神经网络\n",
    "        \n",
    "        参数:\n",
    "        layer_sizes -- 包含各层神经元数量的列表，如[784, 30, 10]表示:\n",
    "                       - 输入层784个神经元\n",
    "                       - 隐藏层30个神经元\n",
    "                       - 输出层10个神经元\n",
    "        \"\"\"\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        \n",
    "        # 初始化偏置向量(每层一个向量，从第二层开始)\n",
    "        # 使用标准正态分布随机初始化\n",
    "        self.biases = [np.random.randn(y, 1) for y in layer_sizes[1:]]\n",
    "        \n",
    "        # 初始化权重矩阵(每层之间一个矩阵)\n",
    "        # 使用标准正态分布随机初始化\n",
    "        self.weights = [np.random.randn(y, x) \n",
    "                       for x, y in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"\n",
    "        前向传播计算网络输出\n",
    "        \n",
    "        参数:\n",
    "        a -- 输入向量(二维numpy数组)\n",
    "        \n",
    "        返回:\n",
    "        网络的输出\n",
    "        \"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \"\"\"\n",
    "        使用随机梯度下降训练网络\n",
    "        \n",
    "        参数:\n",
    "        training_data -- 训练数据集，格式为[(x, y), ...]，其中:\n",
    "                        x是输入向量，y是期望输出向量\n",
    "        epochs -- 训练轮数\n",
    "        mini_batch_size -- 小批量大小\n",
    "        eta -- 学习率\n",
    "        test_data -- 测试数据集(可选)，用于评估训练效果\n",
    "        \"\"\"\n",
    "        n = len(training_data)\n",
    "        if test_data:\n",
    "            n_test = len(test_data)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # 每轮训练前打乱数据顺序\n",
    "            random.shuffle(training_data)\n",
    "            \n",
    "            # 将训练数据分成多个小批量\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)\n",
    "            ]\n",
    "            \n",
    "            # 用每个小批量更新网络参数\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            \n",
    "            # 输出训练进度\n",
    "            if test_data:\n",
    "                accuracy = self.evaluate(test_data) / n_test\n",
    "                print(f\"Epoch {epoch}: Accuracy {accuracy:.2%}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch} complete\")\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "        用一个小批量数据更新网络参数\n",
    "        \n",
    "        参数:\n",
    "        mini_batch -- 小批量数据，格式为[(x, y), ...]\n",
    "        eta -- 学习率\n",
    "        \"\"\"\n",
    "        # 初始化梯度累加器\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # 计算小批量中所有样本的梯度总和\n",
    "        for x, y in mini_batch:\n",
    "            # 对每个样本计算梯度\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            \n",
    "            # 累加梯度\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        \n",
    "        # 更新参数: 参数 = 参数 - (学习率/批量大小) * 梯度\n",
    "        self.weights = [w - (eta/len(mini_batch)) * nw\n",
    "                       for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta/len(mini_batch)) * nb\n",
    "                      for b, nb in zip(self.biases, nabla_b)]\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        反向传播算法\n",
    "        \n",
    "        参数:\n",
    "        x -- 输入样本\n",
    "        y -- 期望输出\n",
    "        \n",
    "        返回:\n",
    "        包含梯度值的元组 (nabla_b, nabla_w)，格式与self.biases和self.weights相同\n",
    "        \"\"\"\n",
    "        # 初始化梯度\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # 前向传播(保存中间值)\n",
    "        activation = x  # 当前层激活值\n",
    "        activations = [x]  # 存储各层激活值\n",
    "        zs = []  # 存储各层加权输入(z = w*a + b)\n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        # 反向传播\n",
    "        # 输出层误差\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        # 从倒数第二层开始反向传播误差\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        \n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"\n",
    "        评估网络在测试数据上的表现\n",
    "        \n",
    "        参数:\n",
    "        test_data -- 测试数据集，格式为[(x, y), ...]\n",
    "        \n",
    "        返回:\n",
    "        正确分类的数量\n",
    "        \"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                       for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"\n",
    "        二次代价函数的导数\n",
    "        \n",
    "        参数:\n",
    "        output_activations -- 网络输出\n",
    "        y -- 期望输出\n",
    "        \n",
    "        返回:\n",
    "        输出误差\n",
    "        \"\"\"\n",
    "        return (output_activations - y)\n",
    "\n",
    "\n",
    "# 辅助函数\n",
    "def sigmoid(z):\n",
    "    \"\"\"S型激活函数\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"S型函数的导数\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8d1955f-f57b-4b81-ab82-64fba5618d28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T05:18:32.760418Z",
     "start_time": "2025-04-27T05:18:29.233040Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d6a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Ne"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "187px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
