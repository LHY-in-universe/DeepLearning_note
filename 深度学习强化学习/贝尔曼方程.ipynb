{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da33b8eb",
   "metadata": {},
   "source": [
    "# 贝尔曼方程\n",
    "$\\begin{aligned}\\nu_{\\pi}(s)&=\\mathbb{E}_{\\pi}[R_{t}\\mid S_{t}=s]+\\gamma\\mathbb{E}_{\\pi}[G_{t+1}\\mid S_{t}=s]\\\\&=\\sum_{a,s^{\\prime}}\\pi(a\\mid s)p(s^{\\prime}\\mid s,a)r(s,a,s^{\\prime})+\\gamma\\sum_{a,s^{\\prime}}\\pi(a\\mid s)p(s^{\\prime}\\mid s,a)\\nu_{\\pi}(s^{\\prime})\\\\&=\\sum_{a,s^{\\prime}}\\pi(a\\mid s)p(s^{\\prime}\\mid s,a)\\{r(s,a,s^{\\prime})+\\gamma\\nu_{\\pi}(s^{\\prime})\\}\\end{aligned}$\n",
    "\n",
    "\n",
    "表示状态s 的价值函数和下一个可能状态是s‘ 的价值函数之间关系的式子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fcc974",
   "metadata": {},
   "source": [
    "# 行动价值函数 action-value function OR Q-function\n",
    "$q_\\pi(s,a)=\\mathbb{E}_\\pi[G_t\\mid S_t=s,A_t=a]$\n",
    "Q函数在t时刻的状态s下采取行动a, 并从t+1时刻开始根据策略$\\pi$采取行动, 此时得到的收益期望值是$q_{\\pi}(s,a)$\n",
    "\n",
    "注意到Q-function只是在状态价值函数选取了一个特定的行动a, 所以有关系\n",
    "$\\nu_\\pi(s)=\\sum_a\\pi(a\\mid s)q_\\pi(s,a)$\n",
    "\n",
    "# 对应的贝尔曼方程 \n",
    "$q_\\pi(s,a)=\\sum_{s^{\\prime}}p(s^{\\prime}\\mid s,a)\\left\\{r(s,a,s^{\\prime})+\\gamma\\sum_{a^{\\prime}}\\pi(a^{\\prime}\\mid s^{\\prime})q_\\pi(s^{\\prime},a^{\\prime})\\right\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed54869",
   "metadata": {},
   "source": [
    "# 贝尔曼最优方程 Bellman optimality equation\n",
    "对于最优策略 $\\pi_*(a|s)$ 有一下贝尔曼方程:\n",
    "\n",
    "$$\\nu_{*}(s)=\\sum_{a}\\pi_{*}(a\\mid s)\\sum_{s^{\\prime}}p(s^{\\prime}\\mid s,a)\\{r(s,a,s^{\\prime})+\\gamma\\nu_{*}(s^{\\prime})\\}$$\n",
    "\n",
    "最优策略是选择 $)\\sum_{s^{\\prime}}p(s^{\\prime}|s,a)\\left\\{r(s,a,s^{\\prime})+\\gamma v_*(s^{\\prime})\\right\\}$的值最大的行动, 最大值是$\\nu_{*}(s)$\n",
    "\n",
    "所以贝尔曼最优方程可以写为 \n",
    "\n",
    "$$\\nu_{*}(s)=\\max_{a}\\sum_{s^{\\prime}}p(s^{\\prime}\\mid s,a)\\{r(s,a,s^{\\prime})+\\gamma\\nu_{*}(s^{\\prime})\\}$$\n",
    "\n",
    "\n",
    "$$q_*(s,a)=\\sum_{s^{\\prime}}p(s^{\\prime}\\mid s,a)\\left\\{r(s,a,s^{\\prime})+\\gamma\\max_{a^{\\prime}}q_*(s^{\\prime},a^{\\prime})\\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea52e4",
   "metadata": {},
   "source": [
    "基于最优策略的行动价值函数叫做最优行动价值函数 optimal action-value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4dbaaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
